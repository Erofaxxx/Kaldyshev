# Инструкция по установке и запуску лабораторной работы на MacBook Air M1

## Требования

- MacBook Air M1 (Apple Silicon)
- macOS 12.0 или новее
- Подключение к интернету для установки пакетов

---

## Шаг 1: Установка Homebrew

Homebrew - это менеджер пакетов для macOS. Если он еще не установлен:

```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

После установки добавьте Homebrew в PATH (для M1):

```bash
echo 'eval "$(/opt/homebrew/bin/brew shellenv)"' >> ~/.zshrc
eval "$(/opt/homebrew/bin/brew shellenv)"
```

---

## Шаг 2: Установка необходимых инструментов

### Установка GCC (для OpenMP)

```bash
brew install gcc
```

Это установит GCC последней версии (обычно gcc-14 или новее).

### Установка Open MPI

```bash
brew install open-mpi
```

### Установка Python и библиотек для анализа

```bash
brew install python3
pip3 install matplotlib numpy
```

---

## Шаг 3: Проверка установки

Проверьте, что все инструменты установлены корректно:

```bash
# Проверка GCC
gcc-14 --version

# Проверка MPI
mpicc --version
mpirun --version

# Проверка Python
python3 --version
```

---

## Шаг 4: Подготовка к запуску

Перейдите в директорию с лабораторной работой:

```bash
cd /Users/stepankaldyshev/C/par_prog_2/lab_2
```

Дайте права на выполнение для скриптов:

```bash
chmod +x compile.sh run_sequential.sh run_parallel.sh verify.sh
```

---

## Шаг 5: Компиляция программ

Запустите скрипт компиляции:

```bash
./compile.sh
```

Если возникает ошибка с gcc-14, откройте файл `compile.sh` и измените `gcc-14` на актуальную версию (например, `gcc-13` или просто `gcc`).

**Важно:** Для компиляции OpenMP программы используйте:
```bash
gcc-14 -fopenmp -o parallel/task2g_omp parallel/task2g_omp.c -lm
```

Если gcc-14 не найден, используйте:
```bash
/opt/homebrew/bin/gcc-14 -fopenmp -o parallel/task2g_omp parallel/task2g_omp.c -lm
```

---

## Шаг 6: Запуск последовательных версий

```bash
./run_sequential.sh
```

Это выполнит все последовательные версии и сохранит результаты в директории `results/`.

**Примерное время выполнения:** 40-60 секунд для всех программ.

---

## Шаг 7: Запуск параллельных версий

```bash
./run_parallel.sh
```

Это выполнит параллельные версии с разным количеством процессов/потоков (1, 2, 4, 8) и сохранит результаты в `results/parallel/`.

**Примерное время выполнения:** 3-5 минут для всех комбинаций.

### Для MacBook Air M1 (8 ядер):

MacBook Air M1 имеет 8 ядер (4 производительных + 4 энергоэффективных). Для лучших результатов:

```bash
# Ограничить использование только производительных ядер
export OMP_NUM_THREADS=4
```

---

## Шаг 8: Проверка корректности

Проверьте, что параллельные версии дают те же результаты, что и последовательные:

```bash
./verify.sh
```

Вы должны увидеть сообщения "✓ Результаты совпадают" для всех тестов.

---

## Шаг 9: Анализ результатов

### Ручной сбор данных времени выполнения

Посмотрите на вывод программ и запишите время выполнения. Откройте файл `analyze_results.py` и замените примерные данные на ваши реальные измерения:

```python
# Замените эти массивы на ваши данные:
reference_times = np.array([10.0, 5.2, 2.8, 1.6])  # Ваши замеры
task1e_times = np.array([12.0, 6.5, 3.5, 2.0])     # Ваши замеры
task2g_times = np.array([11.0, 5.8, 3.0, 1.7])     # Ваши замеры
task3zh_times = np.array([15.0, 7.8, 4.2, 2.5])    # Ваши замеры
```

### Запуск анализа

```bash
python3 analyze_results.py
```

Это создаст графики в директории `results/`:
- `graph_reference_mpi.png`
- `graph_task1e_mpi.png`
- `graph_task2g_omp.png`
- `graph_task3zh_mpi.png`

---

## Шаг 10: Создание скрипта для автоматического сбора данных

Создайте файл `benchmark.sh` для автоматизации измерений:

```bash
#!/bin/bash

echo "=== Бенчмаркинг всех программ ==="
echo ""

# Последовательные версии
echo "Последовательная эталонная программа:"
./sequential/reference 2>&1 | grep "Время выполнения"

echo "Последовательная задача 1е:"
./sequential/task1e 2>&1 | grep "Время выполнения"

echo "Последовательная задача 2г:"
./sequential/task2g 2>&1 | grep "Время выполнения"

echo "Последовательная задача 3ж:"
./sequential/task3zh 2>&1 | grep "Время выполнения"

echo ""
echo "=== MPI программы ==="
for np in 1 2 4 8; do
    echo ""
    echo "--- $np процессов ---"
    echo "Эталонная MPI:"
    mpirun -np $np ./parallel/reference_mpi 2>&1 | grep "Время выполнения"
    
    echo "Задача 1е MPI:"
    mpirun -np $np ./parallel/task1e_mpi 2>&1 | grep "Время выполнения"
    
    echo "Задача 3ж MPI:"
    mpirun -np $np ./parallel/task3zh_mpi 2>&1 | grep "Время выполнения"
done

echo ""
echo "=== OpenMP программы ==="
for nt in 1 2 4 8; do
    export OMP_NUM_THREADS=$nt
    echo ""
    echo "--- $nt потоков ---"
    echo "Задача 2г OpenMP:"
    ./parallel/task2g_omp 2>&1 | grep "Время выполнения"
done
```

Затем:
```bash
chmod +x benchmark.sh
./benchmark.sh | tee benchmark_results.txt
```

---

## Возможные проблемы и решения

### Проблема 1: "gcc-14: command not found"

**Решение:**
```bash
# Найдите установленную версию GCC
ls /opt/homebrew/bin/gcc-*

# Используйте найденную версию, например:
/opt/homebrew/bin/gcc-13 -fopenmp ...
```

### Проблема 2: "mpicc: command not found"

**Решение:**
```bash
# Переустановите Open MPI
brew reinstall open-mpi

# Убедитесь, что PATH содержит Homebrew
echo $PATH
```

### Проблема 3: Программа падает с ошибкой памяти

**Решение:**
Уменьшите размер массивов в программах. Измените:
```c
#define ISIZE 5000
#define JSIZE 5000
```
на:
```c
#define ISIZE 3000
#define JSIZE 3000
```

### Проблема 4: Низкая производительность MPI

**Причина:** MPI на одном компьютере имеет накладные расходы на коммуникацию.

**Решение:** Это нормально. Для демонстрации принципов распараллеливания этого достаточно.

---

## Структура директорий

После выполнения всех шагов у вас будет:

```
lab_2/
├── sequential/           # Последовательные версии
│   ├── reference.c
│   ├── task1e.c
│   ├── task2g.c
│   └── task3zh.c
├── parallel/            # Параллельные версии
│   ├── reference_mpi.c
│   ├── task1e_mpi.c
│   ├── task2g_omp.c
│   └── task3zh_mpi.c
├── results/             # Результаты выполнения
│   ├── result_*.txt
│   └── parallel/
│       ├── result_*_1.txt
│       ├── result_*_2.txt
│       ├── result_*_4.txt
│       └── result_*_8.txt
├── compile.sh           # Скрипт компиляции
├── run_sequential.sh    # Запуск последовательных версий
├── run_parallel.sh      # Запуск параллельных версий
├── verify.sh            # Проверка корректности
├── analyze_results.py   # Анализ и построение графиков
├── ANALYSIS.md          # Теоретический анализ
└── README.md            # Эта инструкция
```

---

## Подготовка отчета

### Что включить в отчет:

1. **Введение**
   - Цель работы
   - Используемые технологии (MPI, OpenMP)

2. **Теоретическая часть**
   - Анализ зависимостей (из файла `ANALYSIS.md`)
   - Векторы направлений и расстояний
   - Обоснование выбранных методов распараллеливания

3. **Практическая часть**
   - Описание реализации
   - Таблицы с временами выполнения
   - Графики ускорения и эффективности

4. **Выводы**
   - Сравнение эффективности разных подходов
   - Влияние зависимостей на производительность
   - Масштабируемость решений

---

## Дополнительные команды

### Просмотр информации о системе:
```bash
system_profiler SPHardwareDataType | grep -E "Chip|Cores"
```

### Мониторинг использования CPU:
```bash
# В отдельном терминале
top -o cpu
```

### Очистка результатов:
```bash
rm -rf results/
rm -f result_*.txt
```

---

## Контрольный список выполнения

- [ ] Установлен Homebrew
- [ ] Установлен GCC с поддержкой OpenMP
- [ ] Установлен Open MPI
- [ ] Установлен Python с matplotlib
- [ ] Скомпилированы все программы
- [ ] Запущены последовательные версии
- [ ] Запущены параллельные версии
- [ ] Проверена корректность результатов
- [ ] Собраны данные времени выполнения
- [ ] Построены графики
- [ ] Подготовлен отчет

---

## Полезные ссылки

- [Open MPI документация](https://www.open-mpi.org/doc/)
- [OpenMP документация](https://www.openmp.org/specifications/)
- [GCC OpenMP руководство](https://gcc.gnu.org/onlinedocs/libgomp/)

---

## Поддержка

Если возникли проблемы, проверьте:
1. Версии установленного ПО (`gcc --version`, `mpicc --version`)
2. Логи компиляции на наличие ошибок
3. Доступную память (`vm_stat`)
4. Количество доступных ядер (`sysctl -n hw.ncpu`)
